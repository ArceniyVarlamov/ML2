

# Анализ ситуации и план действий

## Текущее состояние

Лучший результат: **0.8384** (rank blend). Лидерборд: **0.8495**. Разрыв: **~0.011** — это существенно.

## Диагностика проблем текущих пайплайнов

1. **Слабый feature engineering** — используются только сырые признаки + примитивные агрегаты (row_mean, row_zeros, row_max)
2. **Только CatBoost** — нет diversity моделей (LightGBM, XGBoost, NN)
3. **Наивный отбор фичей** — топ-600 по importance, без учёта корреляций и redundancy
4. **Нет target-специфичной настройки** — одни параметры для всех 41 таргетов
5. **Нет продвинутого стекинга** — только простой rank blend двух похожих моделей

## Стратегия для топ-1

### Приоритет 1: Разнообразие моделей (максимальный эффект на blend)

```python
# Нужно минимум 3-4 разных "движка":
# 1. CatBoost (уже есть)
# 2. LightGBM (другая архитектура сплитов — GOSS, leaf-wise)
# 3. XGBoost (ещё один gradient boosting с другой регуляризацией)
# 4. Нейросеть (TabNet / MLP) — максимально отличается от деревьев
```

### Приоритет 2: Feature Engineering

- **Попарные взаимодействия** топ-числовых фичей (ratio, diff, product)
- **Частотное кодирование** категориальных
- **Target encoding** с proper CV (out-of-fold)
- **Кластерные признаки** (KMeans на числовых → cluster_id как фича)
- **Агрегаты по группам** категорий (mean/std числовых по cat_feature_i)
- **PCA/UMAP компоненты** из extra features

### Приоритет 3: Продвинутый стекинг

- OOF предсказания от каждой модели → второй уровень (logistic regression / LightGBM)
- Weighted rank ensemble с оптимизацией весов по OOF

Хочешь, я начну с конкретного пайплайна? Какой из шагов делаем первым? Я бы рекомендовал начать с **LightGBM пайплайна** — это даст максимальный прирост при бленде с существующим CatBoost, потому что:

1. Быстрее CatBoost в 2-3 раза → можно больше экспериментировать
2. Leaf-wise рост деревьев даёт другие паттерны → сильный diversity для ансамбля
3. Нативная обработка пропусков, но по-другому чем CatBoost