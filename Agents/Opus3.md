

# Анализ ситуации и стратегия выхода на 1 место

## Текущее положение

| Что имеем | Скор |
|---|---|
| Лидерборд #1 | **0.8495** |
| Твой лучший | **0.8384** |
| **Разрыв** | **~0.011** |

Это **огромный** разрыв. Тюнингом гиперпараметров одного CatBoost его не закрыть.

---

## Диагноз: что не так в текущих пайплайнах

**1. Нулевое разнообразие моделей.** Оба пайплайна — CatBoost. Бленд двух CatBoost с похожими параметрами даёт мизерный прирост (+0.0003), потому что ошибки коррелированы. Это тупик.

**2. Примитивный feature engineering.** Три агрегата (row_mean, row_zeros, row_max) — это почти ничего на фоне 2000+ доступных признаков. При этом turbo-скрипт с ними даже *ухудшил* результат.

**3. Один набор фичей на все 41 таргет.** Для разных продуктов важны разные признаки. Универсальный топ-600 — компромисс, проигрывающий таргет-специфичному отбору.

**4. Недоиспользование экстра-признаков.** 600 из 2000+ — это 30%. Остальные 70% информации выброшены.

**5. Нет стекинга.** Простой rank blend — это самая слабая форма ансамбля. Стекинг через мета-модель значительно сильнее.

---

## Стратегический план (приоритет по ожидаемому импакту)

### Шаг 1. Разнообразие моделей (ожидаемый прирост: +0.004–0.007)

Это **самый важный шаг**. Нужно обучить принципиально другие модели:

- **LightGBM** — другой алгоритм сплитов (histogram-based leaf-wise vs CatBoost symmetric trees). Хорошо работает с большим числом фичей, быстрее CatBoost.
- **XGBoost** — третий «голос» в ансамбле, ещё одна имплементация градиентного бустинга.
- Каждая из трёх моделей делает разные ошибки → их ансамбль значительно сильнее бленда двух CatBoost.

Параметры должны **различаться**: разная глубина, разный learning rate, разное число фичей на сплит (feature_fraction / colsample). Это максимизирует декорреляцию.

### Шаг 2. Стекинг вместо бленда (ожидаемый прирост: +0.002–0.004)

Архитектура:
- **Level 0:** CatBoost, LightGBM, XGBoost — каждый генерирует OOF-предсказания (5-fold).
- **Level 1:** Логистическая регрессия (или лёгкий GBDT) обучается на OOF-предсказаниях трёх моделей для каждого из 41 таргета.

Почему это лучше rank blend: мета-модель **учится** оптимальным весам и может давать разные веса разным моделям для разных таргетов.

### Шаг 3. Масштабирование фичей (ожидаемый прирост: +0.002–0.005)

- **Увеличить число экстра-признаков до 1000–1500.** Память позволяет (перевести в float32). Для LightGBM можно и все 2000+.
- **PCA/SVD на всех 2000+ экстра-признаках** → добавить 50–100 компонент как новые фичи. Это сжимает информацию из всех 2000+ признаков в компактную форму.
- **Таргет-специфичный отбор:** для каждого из 41 таргета отдельно выбирать топ-N фичей по mutual information или по importance первичной модели. Это важнее, чем кажется.

### Шаг 4. Feature engineering (ожидаемый прирост: +0.001–0.003)

- **Частотное кодирование** категориальных признаков (frequency encoding) — добавляет информацию о редкости значений.
- **Попарные взаимодействия** между топ-20 числовых фичей (разности, отношения) — CatBoost плохо ловит мультипликативные взаимодействия.
- **Статистики по группам:** среднее/медиана числового признака внутри значения категориального (аналог target encoding, но на самих фичах).
- **Кластерные фичи:** KMeans на топ-50 числовых → номер кластера как категориальный признак.

### Шаг 5. Тюнинг (ожидаемый прирост: +0.001–0.002)

- **Per-target tuning:** для таргетов с низким AUC (< 0.80) — отдельная настройка через Optuna (глубина, lr, l2_reg, число фичей).
- **Разное число итераций:** некоторые таргеты насыщаются на 500 итерациях, другие продолжают учиться до 3000. Early stopping решает это автоматически, но стоит убедиться, что лимит итераций достаточно высок (3000–5000 для CatBoost, 5000–10000 для LightGBM с маленьким lr).

### Шаг 6 (бонус). Продвинутые методы

- **Нейросети:** TabNet или FT-Transformer — ещё один некоррелированный «голос» для стекинга.
- **Pseudo-labeling:** уверенные предсказания на тесте (p > 0.95 или p < 0.05) добавляются в трейн. Особенно помогает для редких таргетов.
- **Multi-target корреляции:** OOF-предсказания по другим таргетам как фичи (осторожно, с правильной CV-схемой).

---

## Приоритетный порядок действий

```
1. LightGBM 5-fold на тех же 600 фичах → OOF + test preds     [быстро, высокий ROI]
2. XGBoost 5-fold на тех же 600 фичах → OOF + test preds       [быстро, высокий ROI]  
3. Стекинг (LogReg) на OOF трёх моделей                        [быстро после п.1-2]
4. Расширить фичи до 1200+ и PCA-компоненты                    [средняя сложность]
5. Переобучить все три модели на расширенном наборе              [долго, но нужно]
6. Per-target feature selection                                  [кропотливо]
7. Feature engineering (interactions, freq encoding)             [эксперименты]
8. Финальный стекинг всего                                       [финал]
```

---

## Ключевой инсайт

Топ лидерборда почти наверняка использует **ансамбль 3+ разных GBDT** (CatBoost + LightGBM + XGBoost) со **стекингом**, а не один CatBoost. Разрыв в 0.011 — это ровно та дельта, которую даёт переход от одной модели к правильному мультимодельному ансамблю с расширенным набором фичей.