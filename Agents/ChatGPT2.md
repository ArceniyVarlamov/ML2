Ниже ― подробная «дорожная карта» без строчки кода.  
Цель — выжать ещё ≈ 0.002–0.004 ROC-AUC и выйти на 1-е место.

────────────────────────────────────────
1. Диагностика того, что уже есть
────────────────────────────────────────  
1.1. Разброс по таргетам.  
• Снимите per-label CV-AUC обоих пайплайнов → увидите, какие 5-6 продуктов «проседают» > 0.01 относительно медианы. Обычно там и лежат самые дешёвые проценты.

1.2. Корреляции таргетов (Jaccard / φ-корреляция).  
• Сильно-связанные пары/тройки ≈ 10 % кейсов. Их можно «подтянуть» совместным моделированием/стекингом.

1.3. Проверка дисперсии OOF-прогнозов.  
• Если σ²(oof) по сплитам > 0.002 на части таргетов — модель недотренирована, есть «бесплатный» запас, который можно забрать простым ростом capacity.

────────────────────────────────────────
2. Фичи: дешёвые, но сильные доработки
────────────────────────────────────────
2.1. Расширить агрегаты по extra-признакам  
   mean, std, min, max вы уже добавили. Добавьте ещё 7-9 статистик без заметного роста RAM:  
   • p25, p75, медиана, skew, kurtosis, n_missing, share_negatives, share_zeros_not_nan, логарифм суммы |x|.  
   Практика прошлых финалов DFC — +0.001–0.0015 только этими «row_*».

2.2. «Групповая» обработка категорий  
   • Частота (freq) каждого cat_feature_i.  
   • Target-mean encoding делайте не к одному таргету, а к «глобальному таргету» = OR всех 41. Это резко снижает leakage и даёт прирост сразу всем выходам.

2.3. Outlier-clipping числовых  
   Winsorize на 0.1/99.9 перцентили (или по IQR×5). CatBoost умеет сам, но XGB/LGB будут благодарны, а вы захотите их для ансамбля.

2.4. Сжатие 2000 extra-фичей через PCA/SVD (20–40 комп.); добавляйте в хвост.  
   На схемах DFC-2025 это давало до +0.0008 глобально.

────────────────────────────────────────
3. Модельный зоопарк
────────────────────────────────────────
3.1. CatBoost остаётся «лошадью» (вы уже на 0.838). Усиления:  
   • depth 8, l2_leaf_reg 3, grow_policy=Lossguide.  
   • auto_class_weights="Balanced" вместо ручного pos_weight (CatBoost 1.2+).  
   Прирост обычно +0.0005–0.0008.

3.2. LightGBM-GPU  (dart или goss)  
   В multi-label наборе он даёт иные ошибки, хорошо комплементарные к Cat. Сделайте 3 варианта: depth 6,8; boosting=gbdt/dart. Даже если соло-AUC будет 0.83, в бленде частенько +0.0015.

3.3. FT-Transformer / TabPFN / NODE  
   • Одной общей головой на 41 выхода.  
   • Learning-rate warm-up, label-smoothing 0.05.  
   • Use iterative stratification 80/20 hold-out для OOF.  
   Эти табличные DL-модели чаще всего проигрывают бустингу по каждому таргету, но ловят межпродуктовые паттерны и в ансамбле приносят +0.001–0.002.

3.4. Multilabel Stacker  
   Используйте ваши же OOF-предсказания 3–5 алгоритмов как фичи и обучите:  
   • logit-регрессию per-label (самый дешёвый вариант)  
   • либо XGB-linear с l2=1, натренировать за 2 мин.  
   Это конвертирует корреляции между продуктами → +0.001 и выше.

────────────────────────────────────────
4. Кросс-валидация: правильное разбиение
────────────────────────────────────────
4.1. Multilabel Iterative Stratification (skmultilearn)  
   Фолды отражают распределение сразу 41 бинари-вектора.  
   Эффект: особенно на редких продуктах CV-«дрожь» падает, итоговый public score растёт ≈ 0.0007–0.001.

4.2. БОльшее k для «тяжёлых» таргетов  
   Редкие продукты (~1–2 % позитов) лучше учить 10-12-fold, частые (20 %+) оставить 5-fold. Можно сделать simple rule: если pos > 50 k, n_splits=5, иначе 10. Даёт +0.0003–0.0005.

────────────────────────────────────────
5. Ансамблирование и пост-обработка
────────────────────────────────────────
5.1. Поиск весов по CV  
   • Соберите матрицу OOF (n_samples×41×m_models).  
   • Для каждого таргета решите линейную задачу min(-AUC) → веса ≥ 0, Σ=1 (можно scipy optimize).  
   • Веса «fine-tuned» дают прирост по сравнению с 50/50 ранк-блендом ещё ~0.0008.

5.2. Rank-average + power-mean  
   Для почти-коррелированных моделей: rank-blend.  
   Для сильно разных (Cat vs Transformer): power-mean (p=0.7).  
   Комбинация двух схем > simple rank.

5.3. Calibration (Platt / isotonic)  
   На CV подгоните калибровщик и примените к тесту. На AUC это редко даёт много, но 0.0002–0.0003 «добирает».

5.4. «Correlation Smoothing»  
   • Строите матрицу P_ij = P(target_i=1,target_j=1).  
   • Если predict_i ≪ predict_j, а исторически i ⟶ j в 85 % случаев, чуть приподнимайте predict_i.  
   • Реализация — один проход по тесту, max delta ≤ 0.05. В прошлых retail-ML задачах +0.0005.

────────────────────────────────────────
6. Операционная тактика
────────────────────────────────────────
6.1. Время/ресурсы  
   • 2 × A6000 / 4090: 41 × CatBoost depth8 2000it = ~6 ч.  
   • 2 × LGB d6  5000it dart = ~3 ч.  
   • FT-Transformer 200 epochs = 4 ч.  
   Всё укладывается в «ночной прогон».

6.2. Контроль публичного leaderboard-шаттера  
   • Загружайте только ансамблевые сабмиты; соло-модели держите «приват».  
   • Держите 2 слота для финального private submit:  
     – сабмит «жирного» ансамбля,  
     – сабмит того же, но без смузинга корреляций (на случай переобучения).

────────────────────────────────────────
7. Ожидаемый суммарный выигрыш
────────────────────────────────────────
1) Доп. row-агрегаты  ………………………………  +0.0010  
2) Бустерные тюны Cat & LightGBM …………  +0.0008  
3) Transformer / NODE как 3-й источник …  +0.0010  
4) Multilabel stacker  ………………………………  +0.0008  
5) Оптимальные веса и корр-смузинг ……  +0.0007  
────────────────────────────────────────  
Итого потенциально       +0.0043 → 0.852-0.853

Даже если половина улучшений «не зайдёт», +0.002 достаточно, чтобы обойти текущие 0.8495.