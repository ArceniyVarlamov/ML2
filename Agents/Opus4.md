

# Анализ ситуации и план выхода на 1 место

## Где ты сейчас

| Метрика | Значение |
|---|---|
| Твой лучший скор | **0.8384** |
| 1-е место на ЛБ | **0.8495** |
| Разрыв | **~0.011** |

Разрыв в 0.011 по Macro ROC-AUC — это **существенно**. Это не вытянуть одним трюком. Нужна системная перестройка пайплайна.

---

## Диагноз: почему текущий пайплайн упирается в потолок

### 1. Нулевое разнообразие моделей
У тебя по сути **один CatBoost** с минимально отличающимися гиперпараметрами. Блендишь два почти одинаковых сабмита — это даёт жалкие +0.002. Топ-1 почти наверняка блендит 3–5 **принципиально разных** моделей.

### 2. Примитивный feature engineering
Из 2000+ экстра-признаков ты берёшь 600 сырых + 3 агрегата (`row_mean`, `row_zeros`, `row_max`). Это мизер. Нет:
- Target encoding категориальных признаков
- Взаимодействий между топ-фичами (ratios, products)
- Агрегатов по группам категорий
- PCA/SVD-компонент из блока extra-фичей
- Статистик по строкам (std, median, null_count, quantiles)

### 3. Одинаковые гиперпараметры на все 41 таргет
У каждого продукта разный баланс классов, разная сложность — а модель одна и та же. Нет per-target тюнинга.

### 4. Не используются корреляции между таргетами
41 продукт наверняка кластеризуется в группы (карты, счета, услуги). Предсказания одних таргетов — мощные фичи для других. Стекинг между таргетами не используется.

### 5. Нет adversarial validation
Неизвестно, есть ли distribution shift между train/test. Если есть — часть фичей вредит.

---

## Стратегический план (в порядке приоритета)

### Шаг 1. LightGBM как второй движок (ожидаемый буст: +0.003–0.006)
LightGBM и CatBoost ошибаются на **разных** объектах. Rank-blend двух разных GBDT-движков — самый надёжный способ вырасти. Обучаешь LightGBM с 5-fold CV на тех же данных, сохраняешь OOF, блендишь ранками. Это одно даст больше, чем все твои текущие блендинги вместе.

### Шаг 2. Расширенный Feature Engineering (ожидаемый буст: +0.003–0.005)
Ключевые направления:
- **PCA(50–100 компонент)** по блоку extra_features — сжимает 2000+ фичей в информативное пространство
- **Row-level stats**: std, median, null_count, kurtosis по числовым фичам
- **Target encoding** категориальных через CV (OOF-схема, чтобы не было утечки)
- **Frequency encoding** категорий
- **Парные взаимодействия** топ-20 числовых фичей: ratios и разности
- **Агрегаты числовых по категориям**: mean/std num_feature_X сгруппированный по cat_feature_Y для топ-пар

### Шаг 3. XGBoost как третий движок (ожидаемый буст: +0.001–0.003)
Ещё один GBDT с другой архитектурой деревьев. Три движка в ранковом блендинге — стандарт топ-решений.

### Шаг 4. Нейросеть (MLP или TabNet) (ожидаемый буст: +0.001–0.003)
Для по-настоящему разнообразного ансамбля нужна модель другой природы. Простой MLP с 2–3 hidden layers на нормализованных данных. Или TabNet. Или FT-Transformer если хватает ресурсов.

### Шаг 5. Stacking между таргетами (ожидаемый буст: +0.001–0.002)
Двухуровневая схема:
- Level 1: получаешь OOF-предсказания для всех 41 таргета
- Level 2: добавляешь эти 41 OOF-предсказание как фичи и переобучаешь

### Шаг 6. Per-target Optuna тюнинг (ожидаемый буст: +0.001–0.002)
Хотя бы для 10–15 «сложных» таргетов (с низким индивидуальным AUC) подобрать depth, learning_rate, l2_leaf_reg отдельно.

---

## Критичные тактические замечания

- **OOF обязателен для всех моделей.** Без него не получится корректно блендить и стекать. Сейчас у тебя OOF сохраняется только в `17hours.py` — это правильно, нужно делать так для **каждой** модели.

- **Оптимальные веса блендинга** искать по OOF через `scipy.optimize.minimize`, а не руками 50/50 или 75/25.

- **Per-target веса блендинга.** Для одних таргетов лучше CatBoost, для других LightGBM. Оптимизируй веса **по каждому таргету отдельно** на OOF.

- **Число фолдов:** 5 фолдов — золотой стандарт. 10 фолдов дают минимальный прирост при двойном времени. Лучше потратить время на разнообразие моделей.

- **depth=6 для всего** — скорее всего субоптимально. CatBoost часто выигрывает на depth=8 с grow_policy='Lossguide'.

---

## Ожидаемый суммарный результат

| Компонент | Текущий скор | Буст | Итого |
|---|---|---|---|
| Текущий блендинг | 0.8384 | — | 0.8384 |
| + LightGBM | | +0.004 | ~0.842 |
| + Feature Engineering | | +0.004 | ~0.846 |
| + XGBoost/NN | | +0.002 | ~0.848 |
| + Stacking + тюнинг | | +0.002 | ~**0.850** |

Это реалистичная оценка. 0.849–0.851 — это зона борьбы за 1 место.

---

## Главный вывод

Ты сейчас используешь **~30% от потенциала данных**. Одна модель, минимум фичей, наивный блендинг. Топ-1 почти наверняка использует 3+ движка, продвинутый FE и multi-level stacking. Приоритет №1 — **LightGBM + PCA на extra features**, это даст максимальный скачок при минимальных усилиях.